{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark\n",
    "\n",
    "* In memory distributed computing engine\n",
    "* Faster than Hadoop (upto 100x)\n",
    "* Less coding effort (5-10x)\n",
    "* Interactive or batch processing\n",
    "* Built-in rich set of functionalities\n",
    "\n",
    "<img src=\"./images/spark.png\" width=\"400\" height=\"200\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cluster.png\" width=\"800\" height=\"400\" />\n",
    "\n",
    "Image source: https://spark.apache.org/docs/1.1.1/img/cluster-overview.png\n",
    "\n",
    "**Spark Context**: It holds a connection with Spark cluster manager and acts as a client. It is also the coordinator of all spark processes running for the application. \n",
    "\n",
    "**Driver**: A driver is incharge of the process of running the main() function of an application and creating the SparkContext. \n",
    "\n",
    "**A worker**: A worker is any node that can run program in the cluster. \n",
    "\n",
    "**Cluster Manager**: Cluster manager allocates resources to each application in driver program. There are three types of cluster managers supported by Apache Spark â€“ Standalone, Mesos and YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python vs Scala:\n",
    "\n",
    "Spark computation in Python is much slower than in Scala.\n",
    "* Scala is native language for Spark (because Spark itself written in Scala).\n",
    "* Scala is a compiled language where as Python is an interpreted language.\n",
    "* Python has process based executors where as Scala has thread based executors.\n",
    "* Python is not a JVM (java virtual machine) language.\n",
    " \n",
    "Many people ask whether it is really necessary to learn Scala to use Spark. Here's an answer. \n",
    "* If you plan to process serious data across nodes in a large cluster, choose Scala.\n",
    "* However, for most users, Python is sufficient.\n",
    "\n",
    "\n",
    "#### Apache Spark data representations: RDD and Dataframe\n",
    "\n",
    "* **RDD** (Resilient Distributed Database) is a collection of immutable distributed elements of your data, partitioned across nodes in a spark cluster. \n",
    "\n",
    "* **Dataframe**, like an RDD, is a collection of immutable distributed data. Unlike an RDD, data is organized into named columns, like a table in a relational database. \n",
    "\n",
    "* **DataSet** has recently been introduced (will not be covered in the class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD and map, filter, reduce, etc.... \n",
    "We can apply 2 types of operations on RDDs:\n",
    "\n",
    "**Transformation**: Transformation refers to the operation applied on a RDD to create new RDD(s). <br>\n",
    "**Action**: Actions refer to an operation which also apply on RDD that perform computation and send the result back to driver.\n",
    "\n",
    "Example: Map (Transformation) performs operation on each element of RDD and returns a new RDD. But, in case of Reduce (Action), it reduces / aggregates the output of a map by applying some functions (Reduce by key). There are many transformations and actions are defined in Apache Spark documentation, \n",
    "\n",
    "Transformations are *_lazy_*, i.e. are not executed immediately. Only after calling an action are transformations executed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rdd_transformation.png\" width = 600 height = 300/>\n",
    "<img src=\"./images/spark-rdd-trasf-action.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common ways to create RDD\n",
    "* **_parallelize_** creates an RDD from a list\n",
    "* **_textFile_** creates an RDD from a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformations \n",
    "\n",
    "* _**map**(func)_\n",
    "* _**flatMap**(func)_\n",
    "* _filter(func)_\n",
    "* _mapPartitions(func)_\n",
    "* _mapPartitionWithIndex()_\n",
    "* _union(dataset)_\n",
    "* _intersection(dataset)_\n",
    "* _distict()_\n",
    "* _groupByKey()_\n",
    "* _**reduceByKey**()_\n",
    "* _**sortByKey**()_\n",
    "* ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### map vs flatMap and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9), (4, 16)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.map(lambda x: (x, x**2))\n",
    "\n",
    "#print(x)\n",
    "#print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 4, 3, 9, 4, 16]\n"
     ]
    }
   ],
   "source": [
    "y = x.flatMap(lambda x: (x, x**2))\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 9]\n"
     ]
    }
   ],
   "source": [
    "z = y.filter(lambda x: x % 2 == 1)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### distinct, union, and intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "print(z.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = sc.parallelize([1,2,3,4,5])\n",
    "r2 = sc.parallelize([2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(r1.union(r2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(r1.intersection(r2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### operations on (key, value)\n",
    "sortByKey and reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a',1), ('b',2), ('c', 3), ('a', 3), ('b', 4), ('a', 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2), ('c', 3), ('a', 3), ('b', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 3), ('b', 2), ('b', 4), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "print(x.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 14), ('c', 3), ('b', 6)]\n"
     ]
    }
   ],
   "source": [
    "print(x.reduceByKey(lambda x,y: x+y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs Wide Transformations\n",
    "\n",
    "Recall a transformation is applied on an **rdd** and creates another (or possibly many) **rdd**(s).\n",
    "\n",
    "\n",
    "A transformation can be **_narrow_** or **_wide_** depending on whether shuffling of data across partitions is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rdd.png\" width=\"800\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Actions\n",
    "\n",
    "* _count()_\n",
    "* _**collect**()_\n",
    "* _**take**(n)_\n",
    "* _top(n)_\n",
    "* _**reduce**()_\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Computation of Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mcpi.png\" width=\"800\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.144174\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 2000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q) What does **_sc.parallelize(range(0, NUM_SAMPLES)).filter(inside)_** return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text data into RDD\n",
    "\n",
    "Consider the following sample Emails\n",
    "<pre>\n",
    "...\n",
    "Qaddafi's cousin, Col. Ali Qaddafiddam had failed in efforts to recruit fighters among the\n",
    "Egyptian population living immediately across the border with Libya.\n",
    "These individuals added that during the week of February 21 the Libyan Leader spoke to Syrian\n",
    "President Bashir al-Assad on at least three occasions by secure telephone lines. During the\n",
    "conversations Qaddafi asked that Syrian officers and technicians currently training the Libyan\n",
    "Air Force be placed under command of the Libyan Army and allowed to fight against the rebel\n",
    "forces.\n",
    "(Source Comment: Senior Libyan Army officers still loyal to Qaddafi added that On February\n",
    "23, President Assad told General Isam Hallaq, the commander in chief of the Syrian Air Force,\n",
    "to instruct the pilots and technicians in Tripoli to help the Libyan regime, should full scale Civil\n",
    "War breaks out in the immediate future.)\n",
    "On March 2, a military officer with ties to Qaddfi's son Khamis stated privately that the number\n",
    "of Libyan pilots defecting to the opposition has destroyed the morale and professional spirit of\n",
    "the Libyan Air Force at this critical moment, when Tripoli's air superiority is its principal weapon\n",
    "against insurgents. In the opinion of this individual Qaddafi and his senior military advisors are\n",
    "convinced that the European Union and the U.S will impose a no-fly zone over Libya in the\n",
    "immediate future. These advisors believe that the no fly zone will serve as air support for\n",
    "opposition forces. They are also prepared for the Western allies to bomb anti-aircraft facilities in\n",
    "and around Tripoli in preparation for the establishment of the no-fly zone. Foreign Minister\n",
    "Mousa Kousa is convinced that that Russia and Turkey will oppose the move, and may prevent\n",
    "the implementation of the no fly zone.\n",
    "...\n",
    "</pre>\n",
    "\n",
    "### Steps to perform word counts\n",
    "1. read data as an RDD of lines\n",
    "2. filter out empty lines\n",
    "3. split each line into words\n",
    "4. convert each word into (key/value) pair\n",
    "5. reduce them by key \n",
    "6. flip k/v to v/k for sorting\n",
    "7. sort by key in descending order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/wordcount.png\" width=\"1000\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us first load data into RDD\n",
    "lines = sc.textFile('data/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonempty_lines = lines.filter(lambda x: len(x) > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,C05739545,WOW,H,\"Sullivan, Jacob J\",87,2012-09-12T04:00:00+00:00,2015-05-22T04:00:00+00:00,DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545/C05739545.pdf,F-2015-04841,HRC_Email_296,FW: Wow,,\"Sullivan, Jacob J <Sullivan11@state.gov>\",,\"Wednesday, September 12, 2012 10:16 AM\",F-2015-04841,C05739545,05/13/2015,RELEASE IN FULL,,\"UNCLASSIFIED']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonempty_lines.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = nonempty_lines.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,C05739545,WOW,H,\"Sullivan,']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to further remove non-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us now filter out empty lines\n",
    "\n",
    "# And split lines\n",
    "# words = nonempty_lines.flatMap(lambda x: x.split(' '))\n",
    "# print words.take(10)\n",
    "\n",
    "#make k/v pairs\n",
    "\n",
    "# reduce them by key. Note: this generates RDD\n",
    "\n",
    "# flip k/v to v/k for sorting: Note: use map\n",
    "\n",
    "# sort by key in descending order\n",
    "\n",
    "# print top 10\n",
    "\n",
    "# Can we remove stop words?\n",
    "\n",
    "# total counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DataFrame-in-Spark.png\" width=\"600\" height=\"400\" /> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
