{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark DataFrame\n",
    "\n",
    "#### In Apache Spark: \n",
    "* **_DataFrame_** is a distributed collection of rows, where each column is named.\n",
    "* Similar to relational table, Python Pandas object, R dataframe, or Excel sheet with column headers.\n",
    "\n",
    "#### Similar to RDD:\n",
    "* Immuatable: DataFrames cannot be changed, only be transformed.\n",
    "* Lazy evaluation: Task is not executed until an *action* kicks in.\n",
    "* Distributed:Rows and columns are distributed. \n",
    "\n",
    "#### Different from RDD:\n",
    "* DataFrame is designed to process structured data.\n",
    "* Query optimization becomes possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD vs DataFrame\n",
    "<img src=\"./images/rdd_vs_dataframe.jpg\" width=\"600\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Howe to create a DataFrame:\n",
    "* Loading data from a file of various formats: JSON, CSV, XML, ...\n",
    "* Loading data from existing RDD (kind of transformation)\n",
    "* Loading data from various databases\n",
    "\n",
    "It can be created using different data formats. For example, loading the data from JSON, CSV.\n",
    "Loading data from Existing RDD.\n",
    "Programmatically specifying schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DataFrame-in-Spark.png\" width=\"600\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Loading a csv file to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Old Spark requires ....\n",
    "_from pyspark import SparkContext_ <br>\n",
    "_from pyspark.sql import SQLContext_ <br>\n",
    "_from pyspark.sql import Row_ <br>\n",
    "_sqlContext = SQLContext(sc)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividendsDF = sqlContext.read.load('data/NYSE_dividends_A.csv', format='csv', header=True, inferSchema=True)\n",
    "dividendsDF = spark.read.load('data/NYSE_dividends_A.csv', format='csv', header=True, inferSchema=True)\n",
    "dividendsDF.show(10)\n",
    "#dividendsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyPricesDF= spark.read.load('data/NYSE_daily_prices_A.csv', format='csv', header=True, inferSchema=True)\n",
    "dailyPricesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Manipulation\n",
    "\n",
    "You can manipulation a DataFrame in two ways:\n",
    "1. Using functions of DataFrame\n",
    "2. Using SQL after creating/registering a table/view "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Count the number of rows, columns in DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividendsDF.count(), dailyPricesDF.count()\n",
    "#len(dividendsDF.columns), dividendsDF.columns\n",
    "#len(dailyPricesDF.columns), dailyPricesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic statistics (mean, standard deviance, min ,max , count) of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividendsDF.describe('dividends').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select column(s) from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividendsDF.select('stock_symbol', 'dividends').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyPricesDF.filter(dailyPricesDF['stock_price_close'] > 200).show()\n",
    "# equivalentaly\n",
    "# dailyPricesDF.filter(dailyPricesDF.stock_price_close > 200).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy, Aggregate, and OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyPricesDF.groupBy('stock_symbol').agg({'stock_price_close': 'max'}).orderBy('max(stock_price_close)', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries\n",
    "\n",
    "The *sql* function enables applications to run SQL queries and returns the result as a DataFrame.\n",
    "\n",
    "* Global Temporary View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dailyPricesDF.createOrReplaceTempView('daily_prices')\n",
    "dividendsDF.createOrReplaceTempView('dividends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_result = spark.sql('SELECT * FROM daily_prices LIMIT 10')\n",
    "price_result.show()\n",
    "\n",
    "\n",
    "\n",
    "#result.filter(result['stock_price_close'] > 2).show()\n",
    "#result_1 = result.rdd.map(lambda row: (row,1)).toDF()\n",
    "#result_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividend_result = spark.sql('SELECT * FROM dividends')\n",
    "dividend_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join on two views\n",
    "* List the closing prices when companies paid dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = spark.sql('''SELECT div.exchange, div.stock_symbol, div.date, div.dividends,\n",
    "prices.stock_price_close  FROM dividends div INNER JOIN daily_prices prices\n",
    "ON(div.stock_symbol=prices.stock_symbol AND div.date=prices.date) LIMIT 10''')\n",
    "join.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join and GroupBy \n",
    "* What are the maximum, minimum, and average closing procies at the time of dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_group = spark.sql('''SELECT div.stock_symbol, max(prices.stock_price_close) as max_close FROM dividends div \n",
    "INNER JOIN daily_prices prices ON(div.stock_symbol=prices.stock_symbol AND div.date=prices.date)\n",
    "GROUP BY div.stock_symbol LIMIT 10''')\n",
    "join_group.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_group_agg = spark.sql('''SELECT div.stock_symbol, max(prices.stock_price_close) maximum,\n",
    "min(prices.stock_price_close) minimum, avg(prices.stock_price_close) average FROM dividends div \n",
    "INNER JOIN daily_prices prices ON(div.stock_symbol=prices.stock_symbol AND div.date=prices.date) \n",
    "GROUP BY div.stock_symbol LIMIT 10''')\n",
    "join_group_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = join_group_agg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result:\n",
    "    print item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
