{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "### Compute-Intensive vs Data-Intensive Computing\n",
    "\n",
    "Parallel processing approaches can be *generally* classified as:\n",
    "* Compute-intensive (task-parallelism), or \n",
    "* Data-intensive (data-parallelism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Intensive: Titan example\n",
    "\n",
    "<img src=\"./images/md_on_titan.png\" width=\"1000\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Problems and Data Intensive Computing\n",
    "Explosion of Data in size and complexity creates problems:\n",
    "* Instrumentation (how to aquire the data)\n",
    "* Storage (how to store data)\n",
    "* Processing data (how to analyze the data or get the best use of the data)\n",
    "* Management of the data (especially how to grow the data)\n",
    "* And many more, e.g. visualization\n",
    "\n",
    "Data-intensive computing is about efficiently dealing with the BigData workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ornl.png\" width=\"1000\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce and Hadoop Revisited \n",
    "Data-intensive computing uses data parallellism to process large volumes of data typically referred to as big data.\n",
    "Data-intensive applications spend most of their processing time movement and manipulation of data. \n",
    "\n",
    "Typical processing steps:\n",
    "* Partitioning or subdividing the data into multiple segments \n",
    "* Independently processing segments using the same executable application program in parallel \n",
    "* Reassembling the results to produce the completed output data\n",
    "\n",
    "#### MapReduce\n",
    "The MapReduce by Google is designed for data-intensive computing.\n",
    "\n",
    "* Creates a map function that processes a key-value pair associated with the input data to generate a set of intermediate key-value pairs (**Map**)\n",
    "* Merges all intermediate values associated with the same intermediate key (**Reduce**).\n",
    "\n",
    "#### Hadoop\n",
    "* Apache Hadoop is an open source software project which implements the MapReduce architecture. \n",
    "* and HDFS distributed filesystem. \n",
    "\n",
    "Distributed storage system stores large datasets across a large number of data servers, rather than storing all the datasets on a single server. When additional \n",
    "\n",
    "When the amount of data increases, you can add as many servers as you want in the distributed storage system. This makes a distributed storage system scalable and cost efficient, because you are using additional hardware (servers) only when there is a demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop VS Spark\n",
    "\n",
    "Both Spark and Hadoop are open source big data infrastructure frameworks that store and process large data sets, realizing *mapReduce* concept.\n",
    "<p>\n",
    "Spark is said to work faster than Hadoop in many circumstances, however it doesnâ€™t have its own distributed storage system. \n",
    "\n",
    "* HDFS uses MapReduce to process and analyze data. \n",
    "* Hadoop's MapReduce stores all the data in a physical server after each operation. (They do not believe volatile RAM) \n",
    "\n",
    "In contrast, \n",
    "* Spark copies most of the data from disks to a RAM memory (i.e. **_in-memory_**)\n",
    "* In-memory operations reduce the time required to interact with servers and thus makes Spark faster \n",
    "* Spark uses a system called Resilient Distributed Datasets (**_RDD_**) to recover data when there is a failure.\n",
    "\n",
    "\n",
    "#### Hadoop and Spark are not competetor\n",
    "It's not fair to compare Hadoop and Spark, and say \"Spark is better,\" or \"Hadoop is better.\"\n",
    "\n",
    "* Hadoop was designed to handle data that does not fit in the memory \n",
    "* Spark was designed to deal with data that fits in the memory.\n",
    "\n",
    "\n",
    "<img src=\"./images/hadoop_vs_spark.png\" width=\"800\" height=\"300\" />\n",
    "\n",
    "Again, Spark does not have its own system to organize files in a distributed way(the file system). For this reason, in many cases, Spark is installed on top of Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/inmemory.png\" width=\"800\" height=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map, Reduce, Lambda and Python\n",
    "\n",
    "Python offers the **_lambda operator_** or **_lambda function_** to create small anonymous functions, i.e. functions without a name.\n",
    "\n",
    "These anonymous functions are:\n",
    "* created when they are needed, but thrown away upon completion.\n",
    "* mostly used in combination with: *filter(), map(), and reduce()*\n",
    "* added to Python due to the demand from *List* community\n",
    "\n",
    "\n",
    "####  *lambda arg1,arg2,...: expressions*\n",
    "\n",
    "Recall: **.** and **()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def mysum(a,b):\n",
    "    return a + b\n",
    "    \n",
    "def indirect_sum(f, a, b):\n",
    "    return f(a,b)\n",
    "    \n",
    "a = 5; b = 10\n",
    "\n",
    "#print(indirect_sum(mysum, a, b))\n",
    "print(indirect_sum(lambda x,y: x + y, a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _map_ function:\n",
    "* **_map(func, sequence)_**\n",
    "\n",
    "The first argument *func* is the name of a function and the second a sequence (e.g. a list) seq. *map()* applies the function *func* to all the elements of *sequence*. It returns a new list with the elements changed by *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "''''''\n",
    "squared = []\n",
    "for x in items:\n",
    "    squared.append(x ** 2)\n",
    "\n",
    "squared\n",
    "''''''\n",
    "\n",
    "def sqr(x): return x ** 2\n",
    "\n",
    "#squared = map(sqr, items)\n",
    "# or using lambda function\n",
    "\n",
    "square = map(lambda x: x**2, items)\n",
    "print squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _filter_ function:\n",
    "* **_filter(func, sequence)_**\n",
    "\n",
    "As the name suggests **_filter_** extracts each element in *sequence* for which *func* returns **True**. \n",
    "**_func_** therefore should be boolean function that returns either **True** or **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 482 7156 7458 5415  458  269 8558 2585 4947 5642]\n",
      "[482, 7156, 7458, 458, 8558, 5642]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randint(0,10000, 10)\n",
    "print a\n",
    "b = filter(lambda x: x%2==0, a)\n",
    "print b\n",
    "#print len(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _reduce_ function:\n",
    "* **_reduce(func, sequence)_**\n",
    "\n",
    "The reduce function **shrinks** (reduces) *sequence* to a single value by combining elements by iteratively applying *func* to *sequence*.\n",
    "\n",
    "Consider \n",
    "```python\n",
    "seq = [ s1, s2, s3, ... , sn ]\n",
    "```\n",
    "Calling reduce(func, seq) works as follows:\n",
    "1. The first two elements of seq will be passed to func, i.e. func(s1,s2)\n",
    "2. Now *seq = [ func(s1, s2), s3, ... , sn ]*\n",
    "3. *func* will be applied on the previous result and the third element of the list, i.e. **_func(func(s1, s2),s3)_**\n",
    "4. *seq = [ func(func(s1, s2),s3), ... , sn ]*\n",
    "\n",
    "The steps are repeated until one element is left in *seq*, which is returned as the result of *reduce()*\n",
    "\n",
    "Consider the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "a = [47,11,42,13]\n",
    "print reduce(lambda x,y: x+y, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/reduce.png\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
